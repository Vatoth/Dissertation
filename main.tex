\documentclass{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{natbib}
\bibliographystyle{agsm}

\graphicspath{ {./images/} }

\title{Dissertation}
\author{Quentin Sonnefraud}
\date{July 2020}

\begin{document}

\maketitle

\tableofcontents


\section{Introduction}


ReBench is a tool to run and document benchmark experiments. Rebench is maintained by \cite{ReBench:2018}. In this dissertation i'm using a side project, ReBenchDB which is a project started in late 2019 by \cite{ReBench:2018}. The goal of ReBenchDb is to store and facilitate the analytics process with useful performance statistics of the data recorded by ReBench. This dissertation will focus more on using the data produced by rebench and applying work done on warmup analysis by \cite{barrett2017virtual}. I am trying different changepoint algorithms and review them in term of behaviour for some ReBench results. I'm also trying to make better outliers detection than in the paper ( maybe clarify this later) such as for Garbage collection.
In the second part of the dissertation, I will use comparison algorithms between curve to quantify the difference between two benchmarks. This comparison is useful to know if a benchmark as changed in term of behaviour.


\section{Warmup analysis}


Just in time compilation \cite{barrett2017virtual} is that dynamically adapts to the current software workload, compiling "hot" code, i.e. the most used code at a given time (which may represent the entire program, but often only certain parts of the program are processed by Just in time compilation).

The interpreters are fast on ignition. They don't have to go through all the compilation steps before they can execute anything. They start translating the first line and execute it immediately.

Because of this, an interpreter naturally seems to be a good choice to run something like JavaScript. It is important for a web developer to be able to start executing his code as fast as possible. That's why browsers have used interpreters to run JavaScript in the beginning.
The problem with interpreters occurs when you want to run the same code more than once. Typically when you use a loop. In this case, the interpreter has to do the same translation over and over again.

A compiler chooses the opposite compromises.

It needs a little more time at startup because it has to go through all the compilation steps before it can do anything. However, executing the code in a loop is much faster because it is no longer necessary to redo the translation work each time it passes through the loop.
Another difference is that compilers have more time to observe the code and modify it so that it can run faster. These modifications are nothing more and nothing less than optimizations. Since the interpreters do the translation work at the same time as they run the code, they cannot afford to take much time to make optimizations.


In order to overcome the inefficiency of interpreters - having to translate the same code over and over again - browsers have started to add compilers to them.

Each computer and compiler does this in a slightly different way, but the basic idea remains the same. A new piece is added to the Language engine: a code profiler. This profiler observes the code as it runs and makes notes on how many times a piece of code is executed and what types are used.

At first the profiler runs everything through the interpreter.

If the same lines of code are executed a few times, this piece of code is considered "warm". If it is executed very often it is considered "hot".

When a function becomes lukewarm, the JIT will send it to the compiler and will store the result of the compilation.

The basic compiler (BC for Baseline Compiler)

Each line of code is compiled as a "stub". Extracts are indexed by line number and by variable type . If the profiler notices that the same code with the same variable types is executed again, it will simply use the compiled stub.

This helps speed things up. But as I said, a compiler can do much more. It can take the time to figure out the most efficient way to do certain things... to make optimizations. The basic compiler will do some of these optimizations . This should not take too much time, because it doesn't want to freeze the execution for too long.

However, if this code is really hot - if it is executed really often - then it is worth taking the time to do more optimizations.

Those warm and hot phase are what \cite{barrett2017virtual} are interested in, indeed they record every performance for a lot of algorithm then they had identify if those warm and hot phase are really speeding up the code.

\cite{barrett2017virtual} used krun which is an extreme benchmark runner which prepares the machine and the operating system for performing precise measurements of computer programs. Its kind of similar of Rebench because it record some benchmarking data.

\subsection{Background work on Warmup Phase Classification}

Warm-up state.
For this dissertation work I followed a study of \cite{barrett2017virtual} about characterising warm-up stages of popular virtual machines (VM), HHVM (php), Graal, HotSpot JVM (java), and others. 
In the study, various benchmarks were repeatedly executed on each virtual machine to see when the JIT compilation happens and how it affects the performance.
They have observed that JIT compilation usually increases the program performance, however, in very some cases it might even lead to performance degradation or no steady state for the programme. Moreover, we have observe the JIT compilation happens mostly during the first run of a program, which indicates that a "warmup phase".
In \cite{barrett2017virtual} Papers for benchmark states classification they used three states 

\begin{itemize}
    \item The first step is to clean outliers such as garbage collection or JIT compilation as seen above. A garbage collector       is a computer sub-system for automatic memory management.  It is responsible for recycling previously allocated and       unused memory.  
    \item The second step after cleaning those outliers from the data is to applied a changepoint algorithm \cite{killick2014changepoint}
    \item The third step After getting the breakpoints from the changepoints algorithm are to have each segments of the experiments and pass them to the classifcation algorithm that \cite{barrett2017virtual} have made which will classify the experiement with four class (warmup, flat, slow down or no steady-state).
\end{itemize}




\subsection{Clean outliers}


As with the analysis of time series data, we must first determine the outliers ( much larger / smaller than close neighbours), in opinion \cite{barrett2017virtual} this can be compile time of JIT, Garbage collection or other processes that do interfere with the benchmarks.

I applied the same techniques of \cite{barrett2017virtual} to get the same results which is a rolling windows to remove the outliers in the benchmarks like the garbage collection.

\#\#\# Include plot and comment about outliers cleaning, find smarter ways than rolling windows

Anomaly detection consists in highlighting data with a different behavior from the majority of the data. 
Indeed, the detection can be performed in supervised mode in case information on the normality of the data is available. In other words, any data in the learning base can be labeled as normal or abnormal. In semi-supervised mode, the training database is assumed to contain only normal data. 
As for the detection in unsupervised mode, no information on the normality of the data is available. 



\#\#\# ADDDING BACKGROUND

\subsubsection{Unsupervised techniques}
The unsupervised approach is the most suitable for anomaly detection problems and benchmarks data. In this approach, the different algorithms try to distinguish aberrant observations by learning about the data set, without having the labels of the observations: there is no set of observations identified a priori as anomalies. It is therefore not necessary to have labelled data and this simplifies the problem of preparing the data before learning and all the difficulty in constructing labels. In this section, I will briefly present some unsupervised techniques and their application with Rebench Data.




\subsection{Classification}

There was some problem with the classification I adapted the algorithm which works now depending on the size of the benchmarks


\#\#\# Explain what i Did for get it working for rebench data maybe some pseudo code ????

\section{Overview of some changepoint detection algorithm and application of classifcation of warmup analysis}


So first I used a package which allows me to do changepoint analysis which is called ruptures \cite{truong2020selective}, it works well, but after reading the warmup paper I prefer to use the algorithm from R "changepoint" \cite{killick2014changepoint} in order to reproduce the experiment.



\subsection{Changepoint Detection}
In statistical analysis, the changepoint detection problem is a regression problem to estimate the times at which a signal exhibits changes in the distribution. . Classically, changepoint detection is performed for a signal with changes in the mean. More generally, changepoint detection is performed for a signal with changes in the distribution (for example, in the mean and variance). \\

Changepoint  detection can be applied to a sound signal of a program for which we wish to estimate the moments when situations change, to the detection of computer attacks  , or to quality control, here for the rebench Data it help to classify behaviour in warmup analysis. \\

This next will deals with the problem of detecting changepoint retrospectively (known as offline) when all the signal data are available. This context is different from a real time detection (online) where the signal data arrive on the fly it could be interessting to classify on the fly after reaching  and classifying a stabilisation of a benchmark and stoping recording the benchmark data in order to save time , but online detection is less able to detect precisely the moment of rupture.

\subsection{Offline detection Parametric and Non Parametric}
 
 
Theire two big families of offline detection algorithm

\subsubsection{Parametric}

Advantages: speed of calculation, ease of interpretation and prediction,
good speed of convergence, possibility of validation;
Disadvantages: a priori choice of known functions (example: degree of polynomial), adapted to a limited class of tendencies.

\subsubsection{Non-parametric}


I Advantages: adaptability, not a priori on the type of trend;
I Disadvantages: limits in interpretation, validation and prediction,
choice of window / smoothing parameters.

\subsection{Package Changepoint}

\subsection{Package Ruptures}



\#\#\# Include benchmarks

Of course, there are very different ways of proceeding to analyze this type of signal. One of the most "intuitive" is to cut the series into "homogeneous" segments. This notion of homogeneity can cover, for example, homogeneity in mean, or homogeneity in variance (or homogeneity in mean-variance!).

There is 3 type of detection of Changepoint
Changepoint in mean, Changepoint in variance, and Changepoint mean or variance

\#\#\# Include bencmarks with noise and changement in mean and variance


For example, for the three benchmarks above, it looks like:

the x1 series is affected by changes in the average,
the x2 series is affected by changes in variance, and
the x3 series by changes in mean and variance.

Based on these models, here is the kind of segmentation that could be proposed:

The above results are actually obtained using the changepoint package from Killick and Eckley (2011).

I used 3 different functions:

cpt.mean which is used to detect changepoint in the mean (assuming that the variance is constant)
cpt.var which is used to detect  in the variance (assuming that the mean is constant)
cpt.meanvar which is used to detect changepoint in both the mean and the variance
These three functions are largely configurable. One can thus vary (among others)

The nature of the algorithm used to detect "optimal" breakpoints...
the type of test used to locate the changepoint (one can either assume that the residual distribution of the variables is Gaussian, or one can not make an assumption of distribution and use a non-parametric test)
the type and extent of the penalty applied in order to limit over-segmentation.

These aspects are discussed in Killick and Eckley (2014).

Effect of parameterization

Unsurprisingly, the above functions give very different results depending on how you set them up .

For example here, I can vary the type of algorithm:

\#\#\# Include bencmarks with noise and changement in mean and variance change algorithm

Here I vary the cost function fixed mannualy:

\#\#\# Include bencmarks with noise and changement in mean and variance change cost function

How do I choose a method?
To choose the segmentation algorithm and other associated parameters, we will rely on 3 criteria:

theoretical criteria (which model is best suited to your data? one with constant variance, one where the residuals are Gaussian, one that would account for segments of highly variable lengths, etc.).

Empirical criteria (we choose the method that gives the most "usable" results on real data, and the most correct results on simulated data). This is what i used because i assume that the data from \cite{barrett2017virtual} was verrified and are correct.

Calculation criteria (the method must be computationally applicable for data of a certain size).

Obviously, determining which method is best using multiple criteria of these three types is difficult (if not impossible). Nevertheless, it is possible to discuss it by carrying out (perhaps, all the same, less ambitious!!) the same type of study as \cite{leviandier2012comparison}.


\subsection{Exact segmentation: dynamic programming}



\section{ How to detect the change between benchmarks ? Overview of method to quantify difference between curve}

During this project I have review and try some techniques to quantify the difference between benchmarks and report if changement of behaviour as occure. During this dissertation i prefer to pick one benchmark problem which is the sleeping barber algorithm which is a sharing ressources problem in multi tasking.

\subsection{Sleeping Barber problem}
The program will have to be decomposed into two types of threads. On one side there will be the barber, represented by a single thread looping continuously to see if a customer is waiting, take care of him if necessary or go to bed. On the other side there will be one thread per client, which will simulate the "physical" client. He will try to enter the shop, sit down if he can, get a shave, and disappear.
While our program will have only one barber, there may be as many customers as there are men on the planet (or at least as much space in memory). The client threads will therefore pile up, waiting for space to become available in the waiting room, and then the barber will take care of them.

\subsubsection{ What does the barber do?}
The thread symbolizing the barber will therefore be unique. It will be started when the program is launched, before any customer, and will loop back on itself for all eternity.
Here's what our barber will spend his life doing:

\begin{itemize}
    \item Is there anyone in the waiting room? If there is, I'll take care of him, if not, I'll go to bed;
    \item When a customer is there I take him to the chair;
    \item I shave him;
    \item I give him the day off.
    \item Of course, when we get to the end, we loop back in.
\end{itemize}

\subsubsection{ What does the customer do?}

Here are the actions that the thread symbolizing each customer will perform. If there are several customers, identical threads will compete with each other:

\begin{itemize}
    \item I look in the hairdressing salon to see if there's any free space. If there is, I go inside, if not, I wait;
    \item When I'm inside I sit on a chair (and take my comic);
    \item I wait until the barber is free;
    \item I get up from my chair (and free a seat) and go into the room;
    \item I let myself trim my beard;
    \item When he's done, I pay and go home.
\end{itemize}

Looking at the difference in number of tasks between the barber's and the client's list of actions, we notice that the client does more things. In fact, the client has to manage an additional resource compared to the barber: the free place in the waiting room when he comes to the entrance of the salon.

\subsection{ Sleeping Barber experiment}

For running the comparaison between two curve I choosed four iterations of the experiment's of the sleeping barber problem implemention. The first one is a record one thousand iteration of the benchmarks and the second and the third one are a record of 5500 iterations their are from the project SOMNS, which is an implementation of the Newspeak Specification derived from the Simple Object Machine,

\#\#\# Explaining what is SOMNS

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{plot_0.png}
    \caption{Benchmark 1 and 2 From sleeping Barber}
    \label{fig:bench_1_2}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{plot_1.png}
    \caption{Benchmark 3 and 4 From sleeping Barber}
    \label{fig:bench_2_3}
\end{figure}

\subsection{Area method}

First i have used area \cite{jekel2019similarity} method to quantify the difference between curve, this an algorithm for calculating the Area between two curves in 2D space
Strictly speaking, the expression area under the curve refers to the area A of the domain delimited by a curve (represented in an x-y diagram) and three straight lines (the x-axis and two vertical lines with abscissa a and b). If the curve has the equation $y=f(x) y=f(x)$, the area is $A=\int _{a}^{b}f(x)\, {d} x$. This area is a true area (e.g. milliseconds for benchmarks) only if the function f has only positive (or zero) values over the interval [a,b] and if both the abscissa and the ordinate are lengths (with the same choice of unit, e.g. milliseconds for benchmarks).
For the purpose of detecting if the behaviour has change between two benchmarks its not interesting because it quantify the area a a whole and area can be equal but not their behaviour

\subsection{Curve Length Measure}

Let us consider in the plane a curve which is the graph of a function f defined on the interval [a, b]. We would like to calculate the length of this curve. To do this, we will approximate the curve by a broken line formed of n segments and calculate the length of this broken line. We will then obtain the exact length of the curve by a limit process.
Theoretically, the length of the curve representative of a
function f over an interval [a; b] (on which it is differentiable) is given by $\int_{a}^{b} \sqrt{1+f^{\prime}(x)^{2}} d x$
This makes it possible to obtain the exact value compared to the initial problem
as well as an approximate value and appreciate the accuracy of the algorithm
according to the value of n.

\subsection{Hausdorff Distance }
The Hausdorff distance is used to account for the maximum deviation between two polylines (L1, L2). By definition, two polylines L1 and L2 are at a Hausdorff distance (dH) from each other of less than d units, if each point of L1 is within d units of at least one point of L2, and if, reciprocally, each point of L2 is less than d units away from each other by at least one
point of L1.

The Hausdorff distance is defined as the greater of the two following components :

\begin{itemize}
    \item d1 which is the largest value of the non-symmetrical distance from L1 to L2,
    \item d2 which is the largest value of the non-symmetrical distance from L2 to L1.
\end{itemize}

\#\#\# Add schematic of L1 and L2 line


\#\#\# This part is unclear i will rewrite the part but its a reminder for me




This distance has the advantage of providing two measurements. Right-of-way lines can thus be compared using the component starting from the most short line. On the other hand, the Hausdorff distance, with the disadvantage of calculating the
distance on the nearest pairs of points and not on homologous points. Homologous points are points that visually correspond to each other. For example, the point in L2 used to calculate d1 is not the Intuitively corresponding to the point of L1, it is simply the closest point. The distance from Hausdorff considers polylines as simple sets of dots unordered. This problem is particularly important for very sinuous or with loops. Small distances can then be sent back to dissimilar lines. Similarly, pairs of dots cannot be considered to be matches. Nevertheless, this particularity has the advantage of
reduce calculation time: the algorithmic complexity of this algorithm is linear. 


This methods seems to be working better because there is no need for the dataset to know if the first clue of the first sequence must match the first clue of the other sequence and that
the last clue of the first sequence must match the last clue of the other sequence (but it must not be its only match).

\subsection{Fréchet distance}

\#\#\#  Developp more this part 



I have presented a discrete variant of the Fréchet distance \cite{frechet_1906} between curves in a metric
space, and they described a simple and efficient algorithm for computing this measure.
Besides its own interest, discrete Fréchet distance may be used for approximately computing the Fréchet distance between two arbitrary curves, as an alternative to using the exact Fréchet distance between a polygonal approximation of the curves or an approximation of this value

\subsection{DTW}

 The DTW finds the best match between a reference (the score) and a signal (the interpretation) by calculating a difference between vectors of the characteristics for each of these signals. The comparison algorithms of chaines are based on the exact correspondence between the reference and the signal and do not take into account, among other things, the imprecision of the pitch estimation due to chords or errors of the height detection algorithm. In addition, the DTW can be used to align continuous multi-dimensional characteristics, for example results from a signal analysis, which allows the partition alignment to be based on parameters such as the number of partitions, the size of the partition, the number of partitions, the number of partitions to be aligned, the number of partitions to be aligned, the number of partitions to be aligned, the number of partitions to be aligned and the number of partitions to be aligned.
The system does not require prior segmentation of the benchmark measurements and does not require prior segmentation of the
signal.
In short, the DTW algorithm consists of three steps:

\begin{itemize}
    \item Calculation of local distances
    \item Dynamic programming to obtain the global optimum \begin{itemize}
        \item a) Calculation of increased distances. Only the minimum predecessors are kept. of each point => local optimum
        \item b) Backtrack to find the minimum distance => global optimum
    \end{itemize}
    \item Result: A shift path that consists of the correspondence of the two equations.

    
\end{itemize}

\subsection{Interpretation of results after running the algorithms}

After Choosing the two benchmarks I ran each algorithm on each experiments of the barber problem.
The 1 \& 2 experiments are not similar in contrary of the 3 \& 4 which are similar by human eyes.
Each output of each algorithm are put inside this tabular

The first result is about the fréchet distance which is the minimal length  between two point of the two curve which is sum  for every point of each experiment that we are comparing.

The second result is the area difference of the two curves (bigger number mean bigger area difference).

The third result is the total lenght difference of the two curves (bigger number mean bigger length difference).

The fourth result is the cost which is the sum of absolute differences, for each matched pair of indices, between their values.

The last result is the sum of each pair of the distance subsets are from each other.

\begin{table}[h!]
\begin{tabular}{|l|c|c|c|c|c|c|}
   \hline
   benchmarks & fréchet distance & area & curve length & DTW & directed hausdorf \\
   \hline
   1 \&  2 & 313 & 81918 & 14 & 68757 & 197\\
   \hline
   3 \& 4  & 295 & 32960 & 48 & 191649 & 7 \\
   \hline
\end{tabular} \\ 
\caption{Comparaison between two inequal benchmarks and two equal benmarks}
\end{table}


As expected the only algorithm that seems to work is the directed hausdorf algorithm because it is not influence by the size of the dataset, indeed as the curve lenght increase the difference of the score increase. 
I have put a treshold of 20 with the directed hausdorf. When the direcred hausdorf value is more than 20 is interesting to look at the benchmark because it means that their is significant difference between the two benchmarks.

\subsection{Conclusion of classification of changement of behaviour between benchmarks}

The validation of an unsupervised classification, as well as the choice of the number of group always remain open questions. On real data, recognized criteria such as the distance difference or the silhouette index are optimal with only two groups, limiting the contribution of such an analysis.

\#\#\# Future work on more example ??



\section{Setup Of Rebench Data}

\#\#\# Part to modify and develop

First thing I did was take the data collected  which is produced by rebench \cite{ReBench:2018}

It's a bit complicated to manipulate the data from the rebenchdb project so I took two days to do crud and a mapping rest of the database I deployed the HTTP server on https://api.rebench.vatoth.dev/ which allows me to manage the data more efficiently.

To deploy the application, I use Docker; this is an open-source software platform to create, deploy and manage containers a container image is. It is a set of light and independent software processes, gathering all the files necessary for the execution of the processes: code, runtime, system tools, library and parameters. They can be used to run Linux or Windows applications. So I dockerised the app to make it easier to deploy in the future. I also use docker-compose, it's a tool that allows to describe (in a YAML file) and manage (in the command line) several containers as a set of inter-connected services. In this application, I describe a set composed of 3 containers: Postgresql, the server node.js for the REST API and a python notebook to present the data. In the beginning, Docker was only used to manage the local working environment, for example, dependencies like PostgreSQL, which allowed me to separate the data from other databases on my computer.

The stack of the project in production is launched like this in production; it builds and launches Docker and does the network mapping between them.

docker-composes -f docker-composes.yml -f docker-composes.production.yml up --build -d

You can see the docker-compose-.yml file in the Github repository \url{https://github.com/Vatoth/master-thesis}. 


\section{Conclusion}

\bibliography{reference}


\end{document}
